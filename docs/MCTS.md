### 引言
蒙特卡洛树搜索（MCTS）结合了**蒙特卡洛采样**和**搜索树**，在建立搜索树的过程中用蒙特卡洛方法进行采样，在指定的范围内进行寻优。  

**搜索树**是根据模拟结果逐步建立起来的，其节点上包含了该节点的**价值**和**访问次数**。  

**价值**是所有涉及该节点的模拟结果的评价的总和，**访问次数**是所有涉及该节点的模拟次数。根据**蒙特卡洛采样**可知当采样次数无限增大时，所有采样的期望值与真实分布的均值无限逼近。
因此节点**价值**越大则表明该节点更优，**访问次数**越大则表明多次采样估计的**价值**与真实价值越接近，那么节点的潜在价值则更低。  

**模拟**时会首先权衡每个节点的**价值**和**访问次数**选择当前树最优的叶子节点作为模拟的起始点，按照自定义的规则进行模拟，每次模拟相当于一次对决策规划行为的随机采样。根据采样结果对模拟起始节点及其所有上层父节点的价值进行估计。  

随着树的高度增加，逐步减小了随机采样的空间。    

**特点：**  
1. MCTS是一种任意时间算法，任意时间算法是一种解的质量随着时间的增加而逐步提高的算法。
从本质上讲,任意时间算法是一种反复求精算法,它可以很快地生成一个不精确解,然后经过若干次重复过程逐步提高解的质量。  

### 背景
#### 马尔科夫决策过程与强化学习
##### 马尔可夫性
马尔可夫性即未来的状态只依赖于当前时刻状态。
<center>
**P(s<sub>t</sub>|s<sub>t-1</sub>)** = **P(s<sub>t</sub>|s<sub>t-1</sub>,...,s<sub>0</sub>),t=1,2,...**
</center>  
**P(s<sub>t</sub>|s<sub>t-1</sub>)**为某个时刻的随机变量**s<sub>t</sub>**与前一时刻随机变量**s<sub>t-1</sub>**的条件分布,
**P(s<sub>t</sub>|s<sub>t-1</sub>,...,s<sub>0</sub>)**为随机变量**s<sub>t</sub>**与所有历史时刻随机变量**s<sub>t-1</sub>**的条件分布。
##### 状态转移矩阵
具有马尔科夫性的离散状态之间的转换概率**P<sup>a</sup>(s<sup>'</sup>|s)=P(s<sub>t+1</sub>=s<sup>'</sup>|s<sub>t</sub>=s)**可以组成一个nxn矩阵,n为离散状态的数量。
矩阵反映了当前任意状态转换为将来任意状态的概率。
##### 马尔科夫链/马尔可夫过程
马尔科夫链由一系列满足马尔可夫性的随机状态**S={s<sub>1</sub>,s<sub>2</sub>,...s<sub>n</sub>}**和状态转移矩阵**P**组成的二元组**(S,P)**描述。  
<center> 
![markov](/mcts/markov.png)
</center> 
至此对于一个**(S,P)**描述马尔可夫过程我们可以根据状态转移矩阵**P**以及初始状态的分布概率P<sub>0</sub>=[P<sub>s1</sub>,P<sub>s2</sub>,...,P<sub>sn</sub>]
(特别的初始状态为**s**<sub>t</sub>也即初始状态分布概率向量中状态**s**<sub>t</sub>的概率为1其余为0)，可以计算后续任意m次转换为任意状态的概率。
<center>
**P<sub>m</sub> = P<sup>m</sup>P<sub>0</sub>**
</center>
随着m的增大P<sup>m</sup>会趋于稳定，这是状态转移矩阵的稳定性。
##### 马尔可夫奖励过程
达到**不同的状态**或者**不同状态之间的转换过程**可能会产生不同的效果,带来不同的收益。在马尔可夫过程中增加奖励就变成了一个马尔科夫奖励过程，
这个过程可以用四元组**(S,P,R,γ)**, 其中R为奖励函数, γ为折扣因子。  
**奖励R(s)=E[r<sub>t+1</sub>|s<sub>t</sub>=s]**, 表征达到状态**s**的平均收益。  
**折扣因子**表征近期利益长期利益之间的平衡关系。 
###### 值函数
根据**R**和**P**可以对所有状态S={s<sub>1</sub>,s<sub>2</sub>,...s<sub>n</sub>}的长期收益进行评估, 可以把这个长期收益称为**价值**。
<center> 
**V(s) = E[G<sub>t</sub>|s<sub>t</sub>=s]**
</center> 
其中**G<sub>t</sub> = R<sub>t</sub>+R<sub>t+1</sub>γ+...+R<sub>t+n</sub>γ<sup>n</sup>**, 这是以某个s为初始状态的随机过程的累计奖励。
从价值的计算公式可以看出，要计算状态s的价值需要穷举以s为初始状态的随机过程，计算它们的累计奖励并求均值，这显然是不可能的。
###### 值函数的计算
**V(s)**  
= E[ G<sub>t</sub>|s<sub>t</sub>=s ]  
= E[ R<sub>t</sub> + R<sub>t+1</sub>γ +...+ R<sub>t+n</sub>γ<sup>n</sup> | s<sub>t</sub>=s]
= E[ R<sub>t</sub> + (R<sub>t+1</sub>γ +...+ R<sub>t+n</sub>γ<sup>n</sup>) | s<sub>t</sub>=s]  
= E[ R<sub>t</sub> | s<sub>t</sub>=s] + E[R<sub>t+1</sub>γ +...+ R<sub>t+n</sub>γ<sup>n</sup> | s<sub>t</sub>=s]  
= E[ R<sub>t</sub> | s<sub>t</sub>=s] + γ **Σ** p<sub>ss<sup>'</sup></sub> E[ R<sub>t+1</sub> +...+ R<sub>t+n</sub>γ<sup>n</sup> | s<sub>t</sub>=s<sup>'</sup>]  
= R<sub>s</sub> + γ **Σ** p<sub>ss<sup>'</sup></sub> V(s<sub>t</sub>=s<sup>'</sup>)  
上式叫做贝尔曼公式。  

1. 动态规划    
利用该公式可以借助动态规划的思想求解V, 当已知R和P时，可以将所有状态的V初始设置为0，然后用该式迭代计算，直至所有状态的价值V趋于稳定。  
2. 解析解  
令**V** = [V(s<sub>t</sub>=s<sub>0</sub>), V(s<sub>t</sub>=s<sub>1</sub>),..., V(s<sub>t</sub>=s<sub>n</sub>)], 
**R** = [R<sub>s0</sub>, R<sub>s1</sub>,..., R<sub>sn</sub>]那么贝尔曼公式可以改写为,   
<center> 
**V = R + γPV**
</center> 
可以解得,  
<center> 
**V = (I - γP)<sup>-1</sup>R**
</center>
对于一些低复杂度的MRP问题, 这样直接计算解析解是非常快的, 但是由于要计算矩阵的逆, 其计算复杂度为O(n^3)。  
3. 蒙特卡洛方法  
方法的核心思想就是进行推演，生成大量随机过程，然后取平均，这其实是一种求期望的方法。  
<center> 
![markov](/mcts/markov_reward.png)
</center> 

马尔可夫奖励过程的四元组**(S,P,R,γ)**可以看作对一个事件的描述，我们可以计算出以s为初始状态的累计奖励最高的状态转移过程, 想要获取最高奖励需要引入动作改变状态之间的转移。
##### 马尔可夫决策过程
通过决策可以促使状态间的转移, 在马尔可夫奖励过程中增加决策和动作的概念, 这个过程就变成了一个马尔可夫决策过程, 用一个五元组**(S,A,P,R,γ)**表示。
这个过程描述了状态S、动作A与收益之间的关系。
  
**A = {a<sub>0</sub>, a<sub>1</sub>,..., a<sub>p</sub>}** 是一系列离散的动作。  
**P<sup>a</sup>(s<sup>'</sup>|s) = P(s<sub>t+1</sub>=s<sup>'</sup> | s<sub>t</sub>=s, a<sub>t</sub>=a)**组成nxpxn的状态转移矩阵**P**。  
**R(s, a)=E[r<sub>t+1</sub>|s<sub>t</sub>=s, a<sub>t</sub>=a]**, **R**为nxp的矩阵。  

这里可以看出状态转移矩阵和奖励取决于状态、动作。  
###### 策略
策略是状态到动作的映射，在某个状态下采取什么样的动作，可以是确定的策略，也可以是一个随机策略(概率事件）。
<center> 
** π(a|s)= P(a<sub>t</sub> = a | s<sub>t</sub> = s) **
</center>

在策略Π下马尔可夫决策过程的状态转移矩阵可以转化为, 

<center> 
**P<sup>π</sup>(s<sup>'</sup>|s) = Σπ(a|s)P<sup>a</sup>(s<sup>'</sup>|s)**
</center>

马尔可夫决策过程的奖励可以转化为,

<center> 
**R<sup>π</sup>(s) = Σπ(a|s)R(s, a)**
</center>

这样将马尔可夫决策过程转化为了策略+马尔可夫奖励过程,   
<center> 
**(S,A,P,R,γ)** -> **Π,(S,P<sup>π</sup>,R<sup>π</sup>,γ)**
</center>
###### 值函数
马尔可夫决策过程有状态值函数和动作值函数：
<center> 
**V<sub>π</sub>(s) = E<sub>π</sub>[G<sub>t</sub>|s<sub>t</sub>=s]**
</center>
<center> 
**q<sub>π</sub>(s, a) = E<sub>π</sub>[G<sub>t</sub>|s<sub>t</sub>=s, a<sub>t</sub>=a]**
</center>
###### 值函数的计算
当给定策略π时, 可以计算状态值函数和动作值函数, 参考马尔可夫奖励过程值函数的计算。
###### 最优值函数/最优策略
最优值函数与最优策略是相互对应的, 在最优策略下得到最优值函数, 从最优值函数中可以得到最优策略。可以通过求解最优值函数得到最优策略。  
策略迭代求解：

1. 给定初始策略π
2. 动态规划求解状态值函数V<sub>π</sub>(s)
3. 根据贝尔曼方程推得，
q<sub>π</sub>(s, a)  
= E<sub>π</sub>[G<sub>t</sub>|s<sub>t</sub>=s, a<sub>t</sub>=a]  
= E<sub>π</sub>[R<sub>t</sub> + R<sub>t+1</sub>γ +...+ R<sub>t+n</sub>γ<sup>n</sup> | s<sub>t</sub>=s, a<sub>t</sub>=a]  
= E<sub>π</sub>[R<sub>t</sub> | s<sub>t</sub>=s, a<sub>t</sub>=a] + γE[R<sub>t+1</sub> +...+ R<sub>t+n</sub>γ<sup>n</sup> | s<sub>t</sub>=s, a<sub>t</sub>=a]  
= R<sub>s</sub><sup>a</sup> + γ **Σ** p(s<sup>'</sup>|s,a)**Σ**π(a<sup>'</sup>|s<sup>'</sup>)q<sub>π</sub>(s<sup>'</sup>, a<sup>'</sup>)  
= R<sub>s</sub><sup>a</sup> + γ **Σ** p(s<sup>'</sup>|s,a)v(s<sup>'</sup>)  
根据上式可以由V<sub>π</sub>(s)计算Q<sub>π</sub>(s, a)
4. 由Q<sub>π</sub>(s, a)可以得到动作价值最大的动作, 得到新的π
5. 重复前面步骤, 直至所有的v<sub>π</sub>(s)=q<sub>π</sub>(s, a)

价值迭代求解：

1. 给定各个状态的初始价值v(s)
2. 利用q(s, a) = R<sub>s</sub><sup>a</sup> + γ **Σ** p(s<sup>'</sup>|s,a)v(s<sup>'</sup>)计算每个状态下每个动作的价值
3. 更新每个状态的价值v(s)等于每个状态下最大的动作价值
4. 重复上述步骤, 直至每个状态的价值稳定
5. 根据Q(s, a)得到每个状态下的最优动作，最终得到π

这两种求解最优π的方式都要求S,A集合以及对应的P, R已知。  
一般情况下，

1. S,A容易获取, 状态s下进行动作a的新状态s<sup>'</sup>也是确定的，但可能状态和动作集合的元素数量很大
2. 而R<sub>s</sub><sup>a</sup>较难确定

##### 强化学习
上述不断迭代更新得到最优策略的过程就是强化学习过程。  
实际问题中一般R<sub>s</sub><sup>a</sup>是未知的，且状态和动作集合的元素数量较大。
因此上述基于R<sub>s</sub><sup>a</sup>已知，且状态和动作有限的迭代求解的方式并不适用这种情况。 

1. 为了应对状态和动作数量较大的情况，可以用一个神经网络模型映射S和A的关系。  
2. 为了应对R<sub>s</sub><sup>a</sup>未知的情况，一般可能通过模拟直至规划问题结束，以模拟结果的评价作为q(s, a)。

策略迭代求解的过程变为：

1. 神经网络模型参数初始化, 相当于给定初始策略π
2. **模拟评估**神经网络模型输出动作的动作价值q(s, a)
3. 根据q(s, a)重新训练神经网络模型
4. 重复上述步骤
